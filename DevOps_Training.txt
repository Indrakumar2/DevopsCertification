Linux Refresher:

Linux was developed by LinusTorvads in 1991

Components:

Bootloader---> A software to manage the booting process of a computer,responsible for loading the operating system in the memory.

Kernel----> it manages the CPU,Memory and Peripheral Devices,It acts as a core interface to communicate b/w the two managing resources.
            effeciently

init System----->exceuted by the linux kernel after the early stages  of bootup completion by the boot loader, it is a subsystem that bootstraps the user
                 space and is charged with controlling the daemons.

Daemons----->These are the background services that either start up during the boot or after logging in to the desktop.
             These Daemons are responsible for managing many parts of the system like, log information, watching for the devices that are inserted or removed & managing the user login.


Graphical Server----->It is refered to as X-server. It render images and shapes on a computer monitor.

Desktop Environment----->The component with which the users interact the most and includes built-in applications.

Applications-----> Desktop environment do not offer the full array of apps, but these applications can installed as per the requirements


Linux Distributions:

it is also refer to as Distros is the term to describe the linux versions.

Ubuntu,Debian, Centos, Fedora, linux mint, opensuse, Archlinux, Elementry os.  


Linux in Devops:
The goal of the Devops is to deliver the software at the faster pace means, building on existing insfrasturcture


Faster Development-------Open Source-------Flexible---------------Scalable

Linux Administration is about managing system operations such as,
----->File Backups and Restorations
------>Disaster Recovery
------>New System Builds
------>Hardware, Software & User Maintainance
------>File System housekeeping
------>Application Installation and Configuartion






Devops overview
Devops is the agile relationship b/w Development and IT operations
Developement+Operations
managing entire Application


                  DEV                 +                 OPS
plan--->Create---->verify---->package---->Release--->configure---->Monitor


Traditional Software Development Approach:

---->Waterfall Model
-----> Agile Model


Waterfall Model
---->It uses a linear and Sequential  Approach for software Development

Stages:

---->Requirement Analysis

                     ------->System Design 
         
                                       ------>Implementation 

                                                        ------->Testing 

                                                                     -------->Deployment
 
                                                                                      ------->Maintainance
Pros:                               Cons:
Easy to use                         Risky & Uncertain
cost Effective                      Not suitable for Complex Projects
Time saving                         Final Product Available only at the end
Easy testing                        not suitable for projects with the changing Requirements
suitable for small Projects         Difficult to make changes in the testing phase
Managerial Control




Agile model:

it is a combination of iterative and incremental process models that focuses process adaptibility and customer satisfaction
by rapid delivery of the working software product.

It breaks the product into small incremental builds,these builds are provided in iterations

working:
----->Product Backlog : it is pretty much all the things that the business wants to get from the software development team. 
Product backlog may have an infinite number of items, but need to decide what we are going to work on in a given period of software development.

------>Sprint Backlog : The above phenomenon is known as Sprint Planning, Leaders from the Software Development team would look at the backlog 
and Decide what will be implemented in a given sprint.

A Passing Sprint means that the items from the product backlog that's been introduced in the sprint have been created, shipped and Deployed

A failing Sprint is one were not all works been done

irrespective of the sprints pass or fail most companies have reterespective on all sprints were things that 
worked and things that didn't work are dicussed  along with figuring out to improve the process

the Scrum team will do plan, Develop, test during the sprint that usually b/w 4-weeks, the scrum team will meet daily, disscusing in the progress of the project


Pros:                                                     Cons
Dynamic Response to requirements                          poor Documentation
Cost Effective                                            Difficult to estimate time and effort for complex projects
Superior Quality Product                                  Risky due to the ever evolving characteristics
Direct Communication b/w the stake holders                Difficult to predict the expected results when requirements are unclear
Best suited for large and long term projects
Minimum Resource Requirement




Relationship b/w Agile and DevOps

using automation tools
Improve collaboration b/w the teams
Create an automated deployment process that moves code from the repository 



DASA DevOPS Principle:

The DevOps Agile Skills Association Defines Certain principle for Devops

Customer-Centric Action

Create with the end in mind

End to End Responsibility

Cross-Functional Autonomous teams

Continous Improvement

Automate Everything you can



Lifecycle

---> Continous developement[Manifests planning and coding, The scope of the project is determined during the planning and developers bulid the code]
               

---> Continous Testing[in this phase teh code has been rigoursly tested for bugs,Here entire testing is automated, Jenkins assist in conducting automation of test procedure so that it saves a lot of time]
                            
-----> Continous integration[it's the essential phase of DevOps, which deals with the software development. Frequent changes are made to the code
                             Building code not only involves complilation but it also includes, UNIT TESTING, INTEGARTION TESTING,CODE REVIEW, CODE PACKAGING]
 
------>Continous Deployment[the new code is deployed frequently and  the configuration management tools are required to achive
Contiainerization also plays a pivtol role in the deployment process These tools helps to maintain consistency throughout ---->development-----staging----testing----production]


------> Continous Monitoring[it includes all operational aspects,it maintains the security and availabilty of the service]




Tool Categories:

SCM, Testing, Software Build,Integartion, CMT and Deployment, Monitoring, Continerization


SCM:
Tracking source Code Changes and history of changes in the code base, and helps resolve conflicts when merging updates from multiple contributors
Git,Github

Software Build:
Maven

Testing:

Selinium

Integration

Jenkins

CMT and Deployment:
Continous Dep & Operations

Codeship, AWSCode Deploy, Terraform , Ansible ,Chef, Puppet, saltstack

Monitoring:

Nagios and Splunk,Amazon cloudwatch, grafana, datadog

Containerization:
It is defined as the form of operating system virtualization through which applications are run in isolated user spaces called containers, 
all using the same shared OS

Docker and Kubernetes

Benifits of Devops:

Assured Rapid Deployment
Balanced Working Environment
Drastic Improvement over quality
Repetative task is Automated
Proactive growth of business
Continous Delivery
Minimal Cost of production
Higher Productivity



DevSecOps:

for security concerns instead of retrofitting the security into the build,it tries to integrate the management of securtiy throughout the development process.

DevSecOps Benefits:
Rapid Cost-Effective Software Delivery
Repeatable and Adaptive Process
improved,proactive Security
Accelerated security vulnerbility patching
automation compatible with modern development




DevOps On Cloud:
Features of cloud ---> 
 
scalability
Elasticity
Availability
Reliability
Manageability
Interoperability
Accessibility
Portability
Perfromance
Optimization


the Cloud service can be categorized in three ways:

Infrastructure as Service (IaaS)--->It is an Instant computing infrastructure service that provides virtualized computing resources over internet.

Platform as Service (PaaS)----->It is a method were users are allowed to build, test, debug, deploy, host, and update their applications in the same environment.

Software as Service (SaaS)---->It is a method of delivering the applications as a service,so that users are free from complex software and hardware management.



Version Control System:

If a project consist of Technical concepts, Collabration b/w different team members, Frequent changes.

A Version control system allows users to keep track of the changes in the software developement projects and enables them to collaborate on the projects

Purpose:

Records changes to set of files

Track every individual change by each contributor

Supports a developers preference and is felxible to use

Faclitates a smooth and continous flow of changes to the code


VCS Stremline in a project:

Allows easy issue tracking

provides a complete long term change history of every file with large file storage

Facilitates easy branching and merging 

Helps in tracking each change made to the software

Tracks the status of each step in the pipeline

Allows Developers to tell each other about different changes pushed to a github repository

Gives real time insights into the codebase 

Offers unlimited free private and Public repositories


Benifits:

Collabrate

Storing versions 

Restoring previous versions 

Understand what happend 

Back up


Types of VCS

Local Version control System: It maintains the track on files within the local system

                               It is very common and simple approch
                                
                               This method very error prone, which means the chances of accidentally writting to the wrong file is higher



Centralised Version Control System: It uses Centralized server to store the files and all the changes in the files are also tracked under the centalized server

          This centralized server also includes all the information of version files and list of clients that checkout files from that central place

Distributed Version Control System: It moves from the client-server approach of the centralized version control system to a peer to peer approach
                                     
                                    The clients can completely clone the repository including its full history

                                    If any server dies any of the client repository can be copied onto the server immediately 



Tools: GitHub, Gitlab, Apache Subversion and Mercurial


Overview of git:

It is type of DVCS

Git allows multiple developers to work together and supports the non-linear developement because of the parallel branches

Git is a software that runs locally the files and their history are stored in the computer

Online hosts are available to store the copy of the files and their revision history

Purpose:

It tracks changes in the source code

User distributed version control tool for source code management

Allows multiple developers to work together

Supports non-linear developement because of its several parallel branches

Benefits:

Has flexible environment 

Facilitates branching and merging 

keeps the files in the secure server

creates and manage remote repositories

Git Repository:

A git repository contains all of the projects files and each file revision history, 
this can be used for discussion and manage the projects work with in the repository


Charcteristics :

The users can have access to the files in the local repository, Whether it's a single or multiple files 

The user can view the public repository by usingt the url of the particular repository

Each repo belongs to a team or a user

    if in the case of User account the user owns the repository
    if in the case of team the team owns it

The repository owner is the only person who can delete the repository

If it is belongs to a team only admin can delete the repository

A code project consists of multiple repositories across multiple accounts, but also can be a single repository from a single account
 
Each repository has 2GB size limit , but it is recommended  to keep the repository to no longer more than 1GB




Life cycle of git:

Repository ------> clone Operation ----->working copy-----> Modify the working copy(Upadate the other developer files 
in the working copy[status differnce operation])------>review the changes (Commit and push changes)------>Commit changes [push operation][
amend and push operation]------>Fix Mistakes [Push the changes]


Git stages 


    Committed                      modified				staged   [Each state relates to a specific git section]


    Working Directory              staging area                         git directory



Working Directory: it is local physical directory were changes can be done to the files

Staging area:  it is the place were the git keeps a reference of all modifications to be sent to the next commit

Git directory: it is the repository that stores all files in a compressed form
  

Untracked files resides only in the workiing directory ie, they don't reflect in the git directory

Modified files resides in both working directory and git directory they have diffrent versions ie, the working directory 
files were changed, but those changes weren't sent to the staging area

Staged files resides in the both working directory and git directory if they are existing files being changed or they 
don't reside in the git directory, ifthey are new file is being added to the respository for the first time

Commited files resides on the both working directory and gir directory with the exact same version.


CI/CD With Jenkins:

Prerequisites-----> Automated bulid tests, Package and deploy operations
                    Quick Pipeline Execution 
                    Quick failure recovery
                    Zero-Downtime deployment
                    Trunk-based Developement 


Jenkins:

Written entirely in java

Benefits: 
Its free

It is  highly portable application

It can be easily installed and configured as per the required needs

It has a vry rich plugin library and global community support

It can be integrated with limitless other applications and tools.


Process of CI in Jenkins:

All the commits are built-------Jenkins CI will check for the builds-------Once the build id fine, the executable code is 
ready for deployment--------Otherwise the team of developers who will commit changess at regular intervals are notified



Job Types in jenkins:

Freestyle Project
Maven Project
External Job
Multi-Configuration project
Pipeline
Folder
Github organization
Multibranch Pipeline

Poll SCM:

It periodically builds SCM, It involves checking the versions at regular intervals for any changes that have been commited 


Webhooks:

It helps to transfer realtime data from one application to another and provide the application a way of consuming new event data from an endpoint.

Bulid Periodically:

It builds the project periodically even if nothing has changed. It might have tests that 
should be run periodically or a DB Clean up jenkins job or any other jenkins job.


Scheduling Jenkins Job:

Scheduled builds simply trigger your bulid job at regular intervals

This statergy can be used for regular night bulids

They can be done with something as simple as a unix cronjob

continous integration requires faster feedback than scheduled bulids

scheduled builds can be used for long running build jobs, where quick feedback is less critical


Syntax:

It uses cron style syntax for scheduling the jobs

              *                  *                   *                              *                       *      
 
         minutes(0-59)  ----  hour(0-23) ----- Day of the Month(1-31)------- month(b/w 1-12) ------ day of the week (0-7 ,where 0&7 are sunday)  



 
notations: 
           '*' --------> represents all possible values for a field  ------- means  once a minute
   
           'M-N' -------> Defines the ranges --------- 1-5 in the DOW field would mean monday to friday

           '/' -------> Defines skips through the ranges ------- */15 in the MINUTE field would mean every 15 minutes

           Comma separated List -------> indicates a list of  valid values ------- 15,45 in the minute field would means at 15 and 45 minutes past evry hour

           Shorthand values  -------- @yearly, @weekly, @Daily, @annualy, @daily, @midnight, and @hourly



Bulid Tools:

Apache ANT & MAVEN

Benefits: 

Easy Configuration 

Easy Support 

Easy Integartion


Maven:

Maven is the bulid tool for managing software dependencies and life cycles.It automatically pulls code from a repository, 
compiles and packages it, and prepares it for the next stage of developement

Tasks:
It can easily create a project 

It is consistent across all the projects

It can connect a project to a source control system

It has the ability to handle muiltiple projects simultaneously

It follows a Standard stratergy


Pros:

It can automatically add all of the project's dependencies by reading a pom file

It can convert a project to war file format easily

It makes it simple to start a project in a variety of contexts and it eliminates the need to manage the dependencies, builds and processing

Cons:

Maven Must be installed on the system before it can be used 

Maven will not be able to add an existing dependency if the maven code for it is not available


Continous integration with maven:
Tutorial need to be done




Automation Testing Framework:

  Overview:

         What AT means?

           Automation testiing is  an application software testing process using software tools to control the execution of tests
           The actual test results are compared with the expected test results



Manual Testing:

It is a form of software testing in which test cases are run by a human rather than using an automated tool.

The Mannual software testing can be summed as:

Time-consuming to test all work flows, fields and bad cases

Hectic for multilingual sites 

Tedious and, as a result, prone to errors


Automation testing can be summed as :

Accelerates the execution of tests 

Helps to increase the test coverage 

Eliminates the need for human interference

Test cases:
 What test cases need to be automated?

The criteria for the test cases, we should keep in our mind,

Test cases are High risk and business critical

Test cases are Executed Repeatedly

They are very monotonous or exhausting to perform manually and those which takes laborious time

Important to note that Automation testing is not appropriate to use in the test cases:

That are ewly designed and have never been manually executed at least once
Test cases whose requirements change frequently
Test cases whose executedin ad-hoc situations


Automation testing process and its popular tools:

Determine the scope automation

Test automation tool selection

Test planning, design & developement

Test Execution

Test automation management



Determine the scope of automation:
The specific area of your application software that will be automated

Guidlines to assess scope:

Empirical(based on experiments and practical experience, not on ideas.)features that are essential for application software
Larger data involvement scenarios of the application software
Identical functionalities shared by the application software products          
The technical feasibility of the appication software

Test Automation Tool selection:
It maybe necessary to evaluate the project specifications attentively by using the scope of automation, 
the right selection of automation tools, testing processes and teams is important for successful automation


Test Planning, Design and Developement:
Once setup an automation strategic framework using this stage and incorporate relevant information such as 
the selection of automation tools as per the requirement such as,

Selection of automation tools as per requirements 
Framework designs and their characteristics
The automation components that are in & out of scope
The preparation of an automated testbed
Schedule and Timeline of scripting and execution


Test Execution:

The automation scripts require input test data, therefore it generates extensive test reports after execution
Automation scripts are executed at this juncture.Before they may be set to execute, 
the scripts require input test data,Therefore it generates extensive test reports after 
execution
the automation tool can be used directly or the test management tool can activate the automation tool


Test Automation Management:

It is an approach is a phase of automation testing that is used to see if the new feature added to the software are working properly.
When new automation scripts are added they must be renewed and maintained to  improve the effectiveness of the automation scripts with each release cycle


Tools in Automation:

'Ranorex' studio is a one-stop solution for automation fuctional user interface tests, regression tests, driven tests, and more
'Testim' is the qucikest way to create durable end-to-end tests, whether theyare codeless, 
programmed or both .It not only allows to write incredibly stable codeless tests,
That the leverage our AI, but it also gives the option to export test as code
'Lamdatest' is used to perform automated  cross-browser testing.They provide a 
selnium grid that is ultra fast,Scalable and secure allowing customers to conduct tests on
over 2000 browsers and operating systems
'Selelinum', is a software testing tool that's used to perform regression testing.Its a 
open source regression testing tool that includes playback recording capabilities


Selenium:

It is an umbrella project that includes a set of tools and libraries that enable and support web browser automation


Features of selenium :

It's a opensource, portable framework used to automate the web applications testing
It's highly flexible when it's come to testing functional & regression testcases
It also Supports cross browsing where the test cases run across various platforms simultaneously
Helps on creating robust, browserbased regression automation suites and perform automation test execution

Selenium Webdriver Architecture:
It is the most Significant component of the selenium tool suite

The test script can be written in any if the supported programming languages and run directly in most contempary web browsers 
using webdriver, includes such as C#, JAVA, Perl, PHP,Phython, Ruby and many others.
  The selenium webdriver is currnetly in the rise with Java and C#

The architecture layout of selenium web driver which is categorized into four main components:
Selenium language bindings also Selenium client libraries-------json wire protocol------Browser drivers------real browsers

The HTTP server, undertake the responsibilty to bridge communication b/w browser driver and real browser.



Selenium Language Bindings: It has language binaries or selenium client libraries to support multiple languages. 
For instance, when we want to access the browser driver in java by using the java bindings

JSON Wire Protocol: JSON is a web based data exchange standard. This manages data structures,such as objects and arrays. 
In a Nutshell Writting and reading data from JSON is Simple.

The JSON Wire protocol is data exchange that commuicates b/w a server and a client

Browser Drivers: Selenium uses exclusive drivers for each browser in establishing a safe connection with 
the browser without comprising the internal logic of the browser functionalities.

Real Browser: Selenium Webdriver is compatible with the major browser to arrange podium to the web 
application for testing such as internet explorer, mozilla firefox,Google chrome, safari etc.,




Configuration Management:

It is a system engineering method that ensures a product's characteristics remain consistent during its life cycle

It provides a configuration model of the services, assets,and infrastructure 
by recording the relationship b/w service assets & configuration items, it also ensures 
that the release in a controlled environments and operational use are
completed on the basis of former approvals.

It may cover Non-IT assets and work products used to develop services and configuartion items are required to support services that aren't listed as assets

Any change in configuartion can dramatically impact : the performance ,the security, the functionality of the code in the underlying of the IT assets

Any component that requires management to deliver IT service is considered part of scope of the configuration management, 
this process maintains information about any CI required to deliver an IT Service, 
including its relationship and this information is managed throughout lifecycle of the CI

So, the objective of configuration management is to define and control the components of the IT service and its infrastructure, 
and to maintain accurate configuration information

The Configuration Management application : identifies, defines and tracks an 
organization's configuration items (CI) by creating and managing records for those items

other service management applications can then access these records  from central repository

Eg:If we are creating an incident, you can take all the data related to hardware components from 
configuration management. Next, we can use the same information to create a new incident. Access to configuartion 
management significantly reduces the time spent to reslove the incident and alerts us to other future risks triggered by other 
component relationships and dependencies defined in the database

Roles of Configuration Management Tools:

Configuration management tools enabled changes and deployments to be faster, repeatable, scalable, predictable, 
and able to maintain the desired state, which brings controlled assests into an expected states.

Advantages:

It helps to increase the efficency with a well defined configuration process that improves visibility and provides control with the help of tracking

It helps in cost optimization by having a detailed knowledge of all the IT Elements of the configuration, 
which helps to avoid unnecessary duplication 

Track requirements from specification to testing

Identifies and control the software versions

It enhances system and process reliability and correcting the incorrect configurations before a detrimental effect on results

It manages the information about the configuration items

It provides faster restoration of your service if process failure occurs

Facilitates the conduct of functional configuration audits 





Popular Configuration management Tools:

Saltstack, Ansible, Chef, Puppet

Saltstack----->It is a python based open-souce configuartion management tool used to remotely manage configuration items

Ansible------->It is also python based CM tool, also considered as agentless CM tool

Chef-------->It is a ruby based CM Tool having integartion with most of the cloud-based platforms

Puppet------> Ruby DSL-based CM tool used for managing software,systems, and network configuration items


Ansible Overview:
 It's an IT automation engine that automates the cloud provisioning, 
configuration management , application deployment, Intra service orchestration and a variety of IT tasks

Ansible was built from the ground up for multitier deployments, 
and instead of controlling one system at a time it models the IT Architecture by defining how all of the systems interact

Ansible uses playbook to describe automation jobs which are written in YAML-it is a human readable 
data serialization language mainly used for configuration files

Ansible was designed for multi-tier deployment

Ansible models the IT infrastructure by interrelating all the systems

It was constructed for multi-level use from the ground up


Components of Ansible: 

Modules, Module Utilities, Plugins, Inventory, playbooks, the Ansible search path

Modules: Ansible connects nodes and sends scripts known as Ansible Modules
         User can write their own modules 
         Most module accept parameters that define the systems desired state.
Ansible runs these modules by default over ssh and removes them after when they finished
The module library can be stored on any computer and no servers, daemons or databases needed.

Module Utilities: Ansible stores functions as modules utilites when several modules use the same code, 
to reduce duplication and maintainance. However only python or powershell can be used to build module utilities


Plugins: Ansible's core fuctionality is augumented by plugins. Plugins execute on the control mode within 
the /user/bin/ansible method, While modules execute on the target system in separate processes usually on a remote system. 
Ansible usually comes with many useful plugins and can also be created

Inventory: It is a configurtion file where user defines the host information

           It is a text file that contains a list of servers or nodes that the user manages and configures. Usually the servers 
are listed based on their hostnames or ip-addresses.

Playbooks: It is a blueprint for automation tasks, which are complex IT tasks with no human intervention
The Ansible playbooks are simply frameworks, or pre-written codes, that developers can use a startingpoint. 
IT infrastructures, networks, security systems, and developer personas are all routinely automated using ansible playbooks 
The scripts or instructions are written in yaml format


Anisble search path: Modules, module utilities, plugins, playbooks, and tasks can all be stored in different locations.
Several files with the similar or identical names in different locations can be available on the ansible control node, 
if own code will be written to expand ansible's core features
On, any given playbook run, the search path decides which of these files Ansible can find and use.


YAML Scripting: It is simple for humans to read write than other popular data formats like XML, JSON.

Further thare are libraries available in most programming languages for working with yaml, hence Ansible uses it to express ansible playbooks.

For ansible, each yaml file begins with a list of items.

Each item in the list is a key or value pair list, also known as a hash or directory, so need to know how to write dictionaries and lists in YAML

Every YAML file optionally starts with --- (3- dashes) and ends with ...(3-dots)

Rules for creating YAML file:
Yaml is case sensitive.

The file should have .yaml extension.

Yaml does not allow use of tabs while creating YAML file; Spaces are allowed instead 



Terraform Overview:

It is a tool for building, changing and versioning an infrastructure safely and efficiently. Terraform can manage
 existing and popular service provider as well as custom inhouse solutions.
Configuration files describe to terraform the components needed to run a single application or the entire datacentre
Terraform generates an execution plan describing what it will do the desired state and then executes it to build the described infrastructure

Key features:

Infrastructure as code-----> it is described using a High level configuration syntax,
                             This allows a blueprint of your datacenter to be versioned  and treated as would any other code
                             Additionally infrastructure can be shared and reused

As the configuartion in the terraform can be determined what has changed and develop incremental implementation plan

The infrastructure in terraform can manage-----> lowlevel components such as : compute instances, storage & networking
                                                 Highlevel components such as: DNS Entry, SaaS features etc.,


Execution plans: Terraform has  planning step where it generates and execution plan, it shows what Terraform will do when we call apply.
                 This lets to prevent unexpected manipulation of terraform infrastructure.


Resource Graph: Terraform builds a graph of all resources which parallelizes of creation & modification of 
any non-dependent resource, because of this terraform builds the infrastructure as efficiently as possible and 
operates can get inside the dependencies in their own infrastructure

Change Automation: Complex change sets can be applied to the infrastructure with minimal human interaction, with the previously, 
mentioned execution plan & resources graph we can know exactly what terraform will change in what order avoiding many possible human errors




Containerization with Docker

overview: 

 Container: A container is a piece of software that packages code and all of its dependencies

Containerization is defined as the form of os virtualization through which applications run in a isolated user space called containers, 
all using the same shared os

Benefits:

Portability----> A container separates applications from their host os, allowing them  to perform soomthly in any platform or cloud, 
that is write once run anywhere.

Scalability----> A container application with the service oriented design can handle growing workloads, resulting in increased application sclability.

Faster Developement------> A conatiner can establish a master  version of the image that can be easily deployed on demand resulting in increased appproductivity.

High Productivity-----> Conatiners allows developers to track and make changes to the platforms source code.

Enhanced Security-----> Containerization also increases security since containers adhere to app separation mechanism,making them more secure

Continuity---------> It provides continuity because containers run separately,Therefore the failure of one does not affect the other containers


Virtualization: It allows to run different operating systems on the same physical server's hardware

It uses a software that creates an abstracted layer over computer layer hardware that allows hardware elements of a 
single computer, processors, memory, storage and more can be divided in to multiple virtual computers commonly called Virtual machines VM's.
Each VM's runs its own operating system and behaves like an invdividual computer even though is running in the small portion of actual computer

Virtualization VS Containerization:
                  Virtualization                                                     Containerization

       It uses fundamental elements as the VM's                                 It uses Fundamental unit as containers 

      It allows to runn several os on a physical server's                       It allows to deploy numerous applications on the same server using the same os 
      hardware.   
              
      It replicates the real hardware, such as CPU Cores, RAM, and             It is OS-Level Virtualization and it isn't the replica of the physical 
      represents it as a Separate system                                        machine

      Virtualization is a heavy weight operation that consumes a              Light weight operation
      lot of resources
 
      It provides complete isolation from the host os and other VM's          Containerization provides light weight isolation from the host and other
                                                                              containers but it doesn't provides stronger security as a VM

      VM's have their own OS                                                   Containers uses the host os and share them
                
     Virtulization has a starting time in minutes                              Containerization has a startup time in 
     and allows for slow provisioning                                          milliseconds and allows for faster provisioning                                            

       Virtualization is not portable                                          Conatinerization is portable

Overview : Docker is popular containerization platform for packaging your application along with all of it's dependencies
           Docker keywords are Developed------> Shipped ------>run everywhere


Docker Platform: It is a feature provided by docker to manage the lifecycle of containers 

                 It provides ability to package and run applications in a loosely isolated environment called a container, 
and to manage the lifecycle of the container,it provides tooling and platform   
           
Uses of docker platform: 
  Development: Containers can be used to develop ypur applications and its supporting components.
  Testing:     The application becomes distributed and tested using container.
  Deployment:   When the code is ready, deploy the application as a container or an orchestrated service in your production environment.
  

Advantages:

Docker provides at various stages of the software development lifecycle
Build: By Dockerizing an app into a single or several modules, Developer team can save time, effort and money.
Test: Test engineers may test each conatinerized applications or its components, 
individually with docker without affecting the other parts of the program
Deploy and Maintain: Docker helps teams collabrate more effectively by ensuring 
that the same versions of libraries and packages are utilized throughout the development process.Futher more, 
deploying a previously tested container prevents bugs in the bulid process, allowing for a smooth transition to production.


Features:

Scalable---->Docker containers are particularly scalable due to their light weight nature.
It is allows to configure the system quickly and efficiently.
It allows us to deploy our work loads in less time and less effort.
Its able to reduce the size of the development by delivering a smaller opearting system footprint via conatiner
Increases productivity, simlply it makes technical configuration and application deployment  easier
It provides business benefit  by decreasing the infrastructure and mainatainance expenses of existing application portfolio also
increasing the time to market for new solutions
It allows to run the applications in the secured environment and each container is independent and able to run any type of applications
It contains swarm as a tool to scheduling and clustering docker containers, 
it's frontnd is the docker api which allows us to control it with a vareity of tools it also allows us to manage a 
cluster of docker hosts as if they were single virtual host, 
its a self organised collection of engine that makes plugable backends possible incoming requests for published 
portd on available nodes are routed to an active container using Docker's routing mesh functionality.
It allows the connection even if no tasks is currently running in the node.
Services are the special tasks that allows us to specify the state of conatiner.
Swarm schedules each job and each tasks represents one instance of a container that should be executed.
It enables us to store secrets in the swarm and then chose which services have access to which secrets .


Docker Components:

Docker is build on the client server model.
The Docker client communicates with the Docker daemon, which handles the construction, execution,and distrbution of docker containers.
we can execute the docker client and daemon on the same machine, or we can link a docker client to a docker daemon thats located elsewhere .
The docker client and daemon uses a rest API, Unix sockets, or a network interface to communicate .

Key Components of Docker Architecture:
Docker Host--------> it is the mosts significant component offers a complete evironment for executing and running applications.
It includes the docker daemon, which is one of the most core components. The docker daemon handles docker objects, such as images,Containers, Networks,
and Volumes by lisenting for docker API Request.
To manage docker services daemon can communnicate with other daemons
 
Docker Client------>Docker users can use a client to interact with the docker.
When the client executes the docker command, it transmits it to the docker daemon, which executes it.


Docker Registry-----> Docker images are stored in a docker registry .
The docker hub is the public registry that anybody may use and docker is set up by default look for images on it. we can even run a private registry


Docker Objects -------> You create and use images, container, networks, volumes, plugins.

Different types of Objects:
Image -----> An image is a ready only template with docker container creation instruction.
A lot of the time  an image is based on the other image with additional customization

Container ----->A docker container is created when we run a docker image. This container contains all of the application and their environments to start , 
stop or to destroy the docker container  we can the docker API or docker CLI

Volume------>Volumes are used to store the persistant data generated by docker and utilized by docker containers.
Docker manages everything via docker CLI or docker API.

Network------>A way for all the isolated containers to communicate with one another, BRIGE, HOST,OVERLAY, NONE & MACVLAN 
are the five main network drivers in docker.


Docker Engine:
It is the core part of the  a docker. The docker engine is an application that follows a client server architecture. It is installed in the host machine.

Components in the docker engine:

Server---> it is the docker daemon called dockered, that can create and manage docker images, containers, networks etc.,

Rest API---->It is used to instruct the docker daemon what to do.

Docker CLI---->It is client used to  enter docker commands.


Docker Images and Containers:

Images: An image is a ready-only template for building a docker container that is based on a other image that has been modified in some way. we can
either make images or use those that have been developed by others and made available through registry.
To create and use the image, we can create a docker file using a simple syntax for defining the steps required.
A layer in the image is created for each instruction in a docker file.
When we change the docker file and rebuild the image, only those layers which have changed are rebuilt.
When compared to other virtualization technologies, this is the part of what makes so light, tiny and fast.

Images can be divided into Two categories:
Parent Image-----> The FROM directive in the image's docker file specfies thae image is parent image
This image served as the foundation of all the commands.
The FROM scratch directive in a docker file builds a basic image without using any parent images

Base Image----> it does not have any parent image in it is docker file
The FROM Scratch directive is used in a docker file to construct it.

Basic Docker image command

docker image build----> It creates an image from the docker file
docker image history---->It is used to display an image's history.
docker image inspect----->It is used to display extensive information of one or more images.
docker image ls---->to display the list of images
docker image prune-----> it is used to remove the unused image.
docker image pull------>To pull an image or repository from a registry.
docker image push------->To push a image or a repository to a registry.
docker image rm------>to delete one or more images
docker image save----> To save one or more images to a tar archive.



Container: A Container is runnable instance of an image . A container is isolated from other containers and its hosts machine to its large extent.
We can control over how isolated a container's network, storage, and other underlying subsystems are from other containers and the host machine.
When we create or start a container, you define it by image and any configuration options we give.
Any changes to the state of a container that is not saved in persistant storage  are lost when the container is removed.
The docker container is the tool we will see to create, test and deploy the application.
We can create, start, stop, move or destroy a container using docker API or docker CLI.
We can attach storage to a container, connect it one or more networks, or even construct a new image base on its existing state 


Docker Networking:
Networking allows docker containers to commuicate and the outside world via host machine 

Network Driver:

Docker's Networking Subsystem is pluggable and driver based.
By Defaut the docker provides network drivers are:

Brigde--->Containers  running on the same docker daemon server are connected by bridge network.
A software bridge is used in the bridge network allows containers connected in the 
same bridge network to communicate while isolating them from  containers are not connected to the bridge network. 
the docker bridge driver implements rules in the host machine to prevent containers from communicate directly with each other

Host--->When we use the host network mode for a container the stack of that container 
is not isolated from the docker host, the container shares the hostsnetworking namespace and the container not assigned its own ip address

Overlay------> The overlay network driver connects many docker daemon hosts to form a distributed network. 
when encryption enabled, this network sits onthe top of the overlays host-specific networks allowing containers connected to it, 
including swarm service containers to communicate securely.

Macvlan------> Applications particularly legacy applications or network traffic monitoring apps that demand to be connected directly to the physical network.

None--->It is used to completely disable the networking stack on a container.


Summary: Bridge---> When multiple containers need to communicate on the same docker host 
         Host-----> When the network stack should not be isolated from docker host, but other components of the container should be, host 
		 networks are the best option
         overlay----> When containers running on different docker host need to communicate, or when multiple apps require swarm services.
         Macvlan----> When containers are required to look like physical hosts on the network, each with its own MAC address.
                      (This requried when we transition from the VM setup, When we  needing our container to appear as physical host)

Third party network plugins can be used to integrate docker with specialized network stacks.


Docker Registry and Docker Hub:

Docker registry-----> It is a storage and distributions system for named Docker images.Mulitple versions of 
same image may exists each of which is distinguished by its tags. Docker registry is organised into docker repositories,where
our repository holds all the versions of a specific image.
The registry allows docker uses to pull image locally, as well as push new images to registry, given adequate access permissions when applicable.
The user should use a docker registry if they want to ensure that their images are saved in a secure location,
Control over their image distribution, completely integrate image, storage, and distribution tightly into their in-house development workflow. 
Note that the docker registry is compatible with docker engine version 1.6.0 or higher.

Docker Hub-----> Docker hub is the world's largest conatiner image library ansd community.
Over 100,000 container images from software manufactures, open source projects, and the community are available to browse.
Docker hub is a docker hosted-respository, that allows you to search and share the container images with team .

Docker Hub Features: 
Private Repositories: It allows users to push and pull container images.
Automated Build: A user can automatically build container images from GitHub and Bitbucket and push them to docker Hub.
Teams & Organizations:  A user can also manage access to private repositories.
Offical Images: Docker hub allow users to pull and use high quality container images provided by docker.
Publisher Images: It also allows users to pull and use high quality container provided by external vendors. Certified images also include support and guarantee 
compatibility with docker enterprise.
Webhooks: It enables the user to trigger actions after a sucessfull push to a repository to integrate with docker hub with other services.

Docker hub allows to:  
Explore the worlds, largest container image repository, 
easily search more than one million container images, including certified and community provided images.
Get access to free public repositories or choose a subscription plan for private repository.Become a verified publiser.
Run more technology and containers with certified infrastructure, container and plugins.

Docker Compose: It is tool for defining and running multicontainer docker applications.
Compose uses a YAML File to configure the application services.Then, with a single command we can create and start all the services from the configuartion,
Compose is compatible with all environments, including production, staging, development, testing, and continous integartion workflows

Three step process in Compose---> Create a docker file to define your apps environment, so it can be replicated everywhere.
In docker-compose.yml, define the services that make up the app, so that they ca be run in a separate environment.
Run, Docker compose up, and it will start and run the entire app. We can run the docker-compose up using the docker Compose binary.

Characteristics of Docker compose:
Docker compose offers multiple isolated environments on a single host. To isolate environments from each other,Compose uses a project name.
All volumes used by our services is preserved by compose.
To create a conatiner, Compose caches the configuration used. Moreover, Compose reuses the existing containers when we restart a service that has not changed 
and the composed file composed supports variables.
Basically, to customize our composition for different environment or different users, we can use these variables 

Use Case : Developement Environment----> While developing a software, the ability to run an application in an isolated environment and interact with it is crucial.
The compose command line tool can be used to create the environmnent and interact with it.

Automated Testing Environment compose provides a convenient way to create and destory isolated testing environments for the test suite. 																											
We can Create and destroy these environment by defining the full environment in a compose file.

Single Host Deployments: Compose is generally concentrated on developement and testing procedures,although it is progressing on more production-oriented
capabilities with each release.


Docker Compose Commands: 

docker compose build:Used to build or rebuild services
docker compose create: Used to create containers for a service
docker compose down: Used to stop and remove container and networks
docker compose kill: Used to force stop service containers
docker compose ls: Used to list running compose projects.
docker compose pause: Used to pause services 
docker compose ps: command to list containers.
docker compose restart: restart  containers.
docker compose rm: Used to remove stopped service containers.
docker compose start: to start the services
docker compose stop: to stop the contianer services
dokcer compose up: to create and start the containers

CONTINOUS MONITORING

Overview: CM is the ability to detect risk, complilance and Security issues in an operational environment.
It mainly involves monitoring & identifying complilance issues and security risks in each phase of the DevOps cycle.

It acts as an auditing tool where it can navigate through old monitoring data to analyse and improve the performance of the system.

Roles:
It assits in the design and manintainance of the a stabled and reliable system.

It displays application behaviour during peak business hours.

It reduces the cost by acquring precise knowledge of software asset duplication 

It reduces the chance of application going down.

It notifies you if there is a problem with the infrastructure & appication service.

It retrives and analyzes historical data.

Types of continous monitoring :

Real time monitoring, Infrastructure monitoring,  Application Monitoring and network monitoring.

Realtime monitoring: it deals with the monitoring of server CPU statistics, Disk usage and Memory stats, 
Spikes in CPU performance & Input and output count on the server.

Infrastructure Monitoring: It deals with tghe monitoring of , CPU & Memory, Network & Router, 
Appservers , webservers and database servers, IT hardware and Software 

Application Monitoring: It deals with the monitoring of API sucess or Failure, Performance metrics count,
API Accessibility & API HTTP error code

Tools for Continuos monitoring: Nagios,ELK stack, Zabbix, Sensu, NewRelic and Splunk, DataDog, AppDynamics, AWS Cloud watch, Grafana, Paerduty and Ganglia

Nagios: It is  open source continous monitoring tools which is used to monitor the system, network and IT infrastructure.

It is available in two variants: Nagios Core and Nagios XI 

Nagios core is an open source product and Nagios XI is a Licensed version.

Why? 
It can find any tye of network and server issue, 
It monitors the infrastructure actively
It can facilitate automatic problem resolution infrastructure upgrade and Planning 
It trouble shoots server performance issue

Features of nagios: It facilitates customized service checks because of the simple plugin design

It monitors Host resources, Processor load and Disk usage.
It monitors network services; SMTP, POP3,HTTP and Ping
It facilitates parallelized service checks.
It enables automatic log file rotation.
It facilitates proactive problem resolution by event handlers.
It enables the implementation of monitoring hosts 
It enables contact notifications on problem resolution.

Nagios Architecture:
 
Nagios Uses a client Architecture. the Nagios server usually runs a host and the plugin run on the remote hosts,
which are specified for monitoring.

Nagios Architecture is comprised of three components namely, 
Process Scheduler, Plugins, and User Interface.

The scheduler is a component of the server part if Nagios. It sends the signal to execute the plugin at the remote hosts.
The Plugin gets the status from the remote hosts and then sends the data to the process scheduler
The process scheduler updates the user interface and notifications are sent to the admin

Nagios Plugins: Plugins are compiled executable scripts, pearl scripts, and shell scripts, that can be  run from the command line
to check the status of a hosts or service

What does a Plugins do?
Plugins allow the user to monitor databases, operating systems,applicatons, networks equipments,and protocols.
They are standalone extensions to Nagios core.

Nagios Plugin types:
Offical Plugins, Community plugins, and Custom Pluings.

The Offical Plugins are developed and Maintained by the offical nagios plugin team, community plugins are developed by hundereds of Nagios Community Members.
The Custom plugins are developed by the users in order to suit their environment.

Configuring Plugins on remote: 
Nagios plugins monitors hosts, devices, services, protocols and applicatons with Nagios and send the report to the NRP
NRP sends those reports to the Nagios server based on the Nagios server Request.
Nagios and NRP can perform all the processes or with Nagios Plugins.


To monitor the remote hosts on the nagios server two installationa are:required; 
Remote hosts NRP plugin and Nagios plugins, Nagios server NRP plugin.


Monitoring Implementation:Nagios perform monitoring by using default plugins.
There are some default plugins available to monitor devices and  services, including Unix, and Linux, Windows, and network servers; 
routers and Switches; CPU load, Disk usage, Memory usage,and Current Users

Default Plugins: Nagios exchange website, offers a number of addtional plugins developed by users that can be used as per their requirement:
These are "check_http": This plugin is used for monitoring webservers. 
"check_ftp" : Plugin used for monitoring FTP servers
"check_ssh": This plugin is used for monitoring SSH servers
"check_smtp":This plugin is used for monitoring the email servers.
"check_pop": This plugin is used for monitoring the POP3 servers on the email servers
"check_IMAP":This plugin is used for monitoring IMAP4 service on your email servers.

ELK Stack: It is open source distributed monitoring solution  with centralized logging and application performance monitoring 
suitable for almost any structured and unstrctured datasource.

ELK stack is an acronym for three open source projects: Elastic search, Logstash and Kibana
Recently, a new component called beats was included in the ELK stack.

ELK Stack components:

Main components of ELK stack are Elastic Search, Logstash, Kibana and Beats

Elastic Search: Elasticsearch is a distributed search and analytics engine that provides real-time search and analytics for data,structure and unstructured.

Fuctions: 
Store: It can store complex data structures that are serialized as JSON documents.
Index: It can Index the documents almost in real time.
Search: It supports searches through an inverted index 
Analyze:It detects and adds new fields through dynamic mapping, which makes schema-less possible.

Logstash:It is a data processing component of ELK Stack. It collects data from various sources 
and feeds the elastic search or normalizes it to other destinations

Kibana: Kibana is data visualization tools that provides the graphical user interface for elastic search. 
Kibana offers these advantages to its users, simple and easy for beginners to understand. Ease of conversion of visualization and dash boards in to reports
Canvas Visualization help to analyze complex data in an easy way.

Beats: These are lightweight data collectors that are installed directly on the datasource and collect data for specific purposes, which are then sent
to elasticsearch or logstash.
Data Collectors: 
File beat: it sends local file records.
Winlogbeat: It sends the window event logs.
Metricbeat: It sends system or application performance metrics.

Efficieny of ELKStack: 
It can view stats to detect application defects.
It collects logs from servers and applications that can be analysed for improvement. 
It can provide backup of monitoring stats and able to store the monitoring system.
It is combaination of elasticsearch for searching data, log stash to process and store various status, and kibana to visualize stats on 
front end application.
It has a set of utilities which can provide the most powerfull business analytics. 




Continous orchestration using Kubernetes:

Container Orchestration: It automates the deployment, Management, scaling and Networking of containers.

It is useful for enterprises to deploy and manage multiple conatainers and hosts.

purpose of conatainer orchestration:

A container orchestrator is a system that automatically deploys and manages conatainerized apps.
It responds dynamically to changes in the environment to increase or decrease the deployed instances of the manaed app
It ensures all deployed container instances get updated if a new version of a service is realsed
It dynamically responds to the changes in the environment to increase or decrease the deployed instances of the managed app.
It allows to deploy the same application across different environment without needing redesign it.

Why do we need container orchestration:

conatainer orchestration is essential to automate and manage tasks such as:
provisioning and deployment of containers 
configuring and scheduling of an application in relation to the containers running at.
Allocation of resources between containers. 
redundancy and availability of container,
Scaling up or removing the containers to spread application load evenly across host infrastructure, 
load balancing of service discobery between containers and traffic routing.
Health monitoring of conatiners and hosts 
Secure interactions betwenn containers 

How does container orchestration work:
Starting with the configuration files, describe the configuration of application using either YAML or JSSON file.
The configuration file tells the management tool where to find the container images, hoe to establish a network, and where to store logs
The deployment ofa new container, the orchestration tool automatically schedules to a cluster and chooses the best 
host for it,taking into consideration any define requirements or constraints 
Further container orchestarion tool manages the container life cycle based on the specifications that were determined in the configuaration file. 
It can also be used in any environement that runs containers, including on premises servers and public cloud or private cloud environments.

Container Orchestration tools broadly defined as providing an enterprise level framework for integration and managing container at scale 
These tools aims to simplify container management and provide a framework not only for defining initial container deployment, but also for managing multiple
Containers as one entity, for purposes of availabilty, scaling and networking.

The Lisit of tools: K8's ; Docker Swarm, Apache Mesos, openshit and Hashicorp Nomad.  

Introduction to K8's:
K8's is an powerful opensource system for managing containerized applications in clustered environment.
It aims to provide better ways of managing related distributed components and services access varied infrastructure.
It is also known as Kube.
Note that kubernetes was originally developed by google and then acquired by Cloud Native computing Foundation, CNCF.
K8's acts as a cloud service in major cloud providers, such as EKS in AWS and kubernetes Engine in GCP.

Features:
Automatic Rollouts and Rollbacks:  
Kubernetes has a feature to roll but change to the application or its configuration while monitoring application Health.
If something goes wrong, you can roll back the changes.

Service discovery & Load balancing:
In kubernetes Pods have their own IP address and a single DNS Name is given for a sea of Pods
It enables load balancing 
There is no need to modify application to use any other service discovery mechanism

Horizontal Scaling:
Horizontal scaling enables you to scale your application up and down with a simple command, depending in the requirements.

SelfHealing: Kubernetes allows to replace and reschedule container when nodes die.

Automatic Biin Packing : K8's places conatiners based on their resource requirements and other conatraints, without impacting the availability.
It strikes a balance between critical and best effort workloads.

Batch Execution: It can manage batch and CI workloads using K8's by replacing conatainers that fail.

Storage Orchestration: It allows you to mount a storage system of your choice. we can choose frm local storage, public
cloud provider or use a shared network storage system.

IPv4 and IPv6 dual-stack: K8's allocates IPv4 and IPv6 addresses to pods and services.


Benefits: 
Open source and Modular: It is a fully opensource, Community-led project overseen by the CNCF.

Portability and Flexibility:K8's is highly portable and flexible because it can be used on a variety of  different infrastructure and environement configuration

Multi-Cloud capability: K8's can hosts workloads running on a single cloud as well as workloads that are speard across multiple clouds.

Huge commnunity Support: K8's Community support provides best ideas and practices.

Easy service Organization with pods: The service organization offers pods to create and manage in K8's.

Increased Developer Productivity: With increased developer productivity, teams can deploy multiple times  a day.

Kubernetes Components : The working kubernetes deployment is called as Cluster.

It has two components : The control plane and The compute machine or nodes.

The control plane is responsibel for maintaining the desired state of the cluster and compute machines run the applications and workloads.

Control Plane is the first component of a cluster.
The contril plane components maje global desicions about the cluster like scheduling.
It also helps in detecting and repsonsible to cluster events. 

Components of the control Plane: 

The Kube-apiserver: It is designed to scale hprizontally by deploying more instances. we can run several instances of kube-apiserver and balance
traffic betwenn those instances.

Kube Scheduler: It watches for newly created pods with no assigned node and select a node for them to run on.

Kubecontroller Manager: it runs controller processes. Each controller is a separate process , but to reduce complexity, they are
all compiled into a single binary and run in a single process 

The etcd: It is consistent and highly available key values store used as kubernetes backing store for all cluster data.

The cloud-controller manager is also a control plane component. It helps when one wants to run kubernetes with a specific cloud provider.
It incorporates cloud-specific controll logic and help to link with kubernetes cluster with cloud provider API.
It only runs the controller that are specific to your cloud provider.

The second component of a cluster is node. A K8's cluster needs atleast one compute node , but will normally have many.
Pods are scheduled and orchestrated to run on nodes.

Node Component: 
Kubelet: Kubelet is an agent that runs on each node in the cluster to make sure that conatiners are runnning in a pod.

Kube-Proxy: It implements a network proxy and acts as a load balancer on a kubernetes cluster.
It also helps in redirecting traffic to a specific container in a pod, based on the incoming and IP details

Container Run Time: The container runtime is the underlying software that runs containers on a kubernetes cluster.
It is responsible for fetching, and stopping container images.

Pod: It is smallest and simplest unit in the kubernetes object model.It represents a single instance of an application.
It is the component of the application workload that runs on the worker node.


Kubernetes Architecture: 
It brings together  individual physical or virtual machines into a cluster using a shared network  to communicate between each server 
this cluster is the physical platform  where all K8's components capabilities and workloads are configured 

From a high level the K8's architecture consists of Control plane a distributed storage system to keep the cluster state consistant and
a number of cluster nodes
The control plane is the system that maintains a record of all kubernetes objects. 
It continously manages the object states that responding to the changes in the cluster 
It also works to makes actual state of  system  objects match the desire state all components of control plane can run on a single master node
or can be replicated across multiple master nodes for High availability 
Cluster nodes runs  the container and  are managed by the master node 
The kubelet is the primary and mosts important controller in the K8's
It is responsible for driving the container execution layer typically Docker 
Pods are one of the crucial concepts in the kubernetes as they are the key construct that the developers interact with.
It pacakges up the single application which can consists of multiple containers and stoarge volumes   

Basics: 
The Important components of kubernetes: Pods, Labels and Selectors.

Pods: These are collection of one or more conatainer which shares  storage and network resources and its specifications to run its containers
It is smallest unit in the kubernetes application

Features: Conatiners in the pod can contain multiple applications
pod templates are used to define how pods will be created and deployed 
Pods share physical resources from the host machine in the forms of CPU and RAM & storage.

How to use Pods:
 There two ways: 
 
01---> Pods running in a single container 
02--->Pods running in multiple conatainers

Labels: These are the key value pairs  that K8's attaches to the various objects such as services,Pods and nodes
They are used to organise and select subsets of objects.

Eg of Labels: "release":"stable"   "tier": "frontend"
"track":"daily"  "partition":"Customer A"

Selectors: The selectors are the core grouping primitives. These are used to select a group of objects 
The K8's API currently supports two types of selectors namely Equality based selectors and Set based selectors

Equality based selectors : It uses label keys and values to allow filtering 

Eg: environment=production
     tier !=frontend

Set Based Selectors: It allows filtering keys according to set of values.
Eg: environment in(production,qa)
    tier notion(frontend,backend)
	     partition
		 !partition


further important components:

Controllers: These are the control loops that monitor the state of the K8's cluster and make a request changes were ever needed.

The controller tracks atleast one kubernetes resource type 
These objects have specific fields that represents the desired state

The controller for that resources are responsible for making the current state come closer to that desired state 

It takes care the availability of the Pod and if it fails a replacement pod will be created automatically

It manages pods using labels and selectors to identity the resources 

Types of controller: Replication controller, Daemon set Controller, job controller.

Replica set: 
A Replica Set ensures that a set of replica pods is running at any given time.
It is commonly used to guarantee the availability of the specified number of identical pods.

What Replica set does: 
The Replica Set uses the selector to identify the pods running and based on the result it creates and deletes the pods.
It acquires the Pod if the pod does not have an owner reference and matches the selector of replica sets.

When to use a Replica Set: 
It can be used when we require a custom update orchestration.
Or when do not require update at all 

Deployment: It is controller taht changes the actuall state to the derired state  as specified 
It is used to provide updates to the pods and replica sets 

Uses: It is used to clean up the older replica sets.
Rollout a replica set 
Declare the new state of pods 
Indicate that a rollout is stuck
Apply multiple fixes to pods
Facilitate more load 

Deployment Operations are: Creating a Deployment
Scaling a Deployment 
Updating a deployment Cleaning up a deployment policy 
Rolling up a deployment
Pausing and resuming a deployment 

Services: A service is an abstaraction that defines a logical set of pods as well as a policy for accessing them.
These are defined in YAML that controller tracks atleast one kubernetes types.

How K8's services Work: A kubernetes Services allows internal and external users of the app to communicate with:
Nodes, Pods and Users

Way to access k8's service: DNS Method-->When a new service is established the DNS Server watches the K8's API and its
name becomes available for a quick resolution to requesting apps

Envrionment variable Method: The Kubelet adds environement variables for each active service for each node or pod is operating on in
this approach

Service Types: The cluster IP---> The default service type exposes  service within kubernetes Cluster 
Node port--->It is a static port exposes the services of each node IP.
Load Balancer---> A Cloud provider Load balancer exposes the service externally
External Name---> Returns a CName Record with a value mapping a service to a preset exteranl name field 


Kubernetes Networking: K8's Networking is a service that enables kubernetes is a service that enables kubernetes components to connect with one and another 
as well as with other applications.

It is primaruly concerned with: Containers in a pod communication via loopback utilizing networking.
Cluster networking that allos communicatiion across distinct pods.
The service resource that allows you to expose a pod - based appication so that it can be accessed form outside the cluster.
Services exclusively published for use within the cluster.

K8's Storage:
K8's provides a convinient persistent storage mechanism called persistant volumes.
Volumes serve as crucial abstarction in the kubernetes Storage architecture.

how can we limit storage resource consumption: It is advised to place limits on container uses of storage, to reflect the amount of storage actually available 
in the local datacenter, or the budget available for cloud storage resources.
There are two ways of imiting storage consumption by containers.
01-->Resource Quotas which limits the amount of resources,including storage, CPU and Memory, that can be used by all containers within a k8's namespace,
and  the second is storage classes which limit the amount of storage provisioned to containers in response to a persistant Volume claim.

Best Practices using Kubernetes storage: 
Persistant Volume settings
Resource quotas for namespace
Supports high performance with quality of service definitions.

Kubernetes Configuration: 
When conatainer starts kubernetes uses two sorts of objects to inject configuration data into it which are: Secrets and ConfigMaps

Secrets: It is a little pieces of data, such as a password, a token or a key, that contain a small amount of sensitive information users can create secrets.

ConfigMap: It is an API object used to store non-confidential data in key value pairs.
It can be used by pods as : 
Environment variables 
command-line parameters parameters
Volume configuartion files

Best Practices of using K8's Configuration
Update kubernetes to the latest version
Use pod Security policies to prevnet the use of risky containers or pods
use k8's namespaces to properly isolate the kubernetes resources 
Use network policies to segment and limit pod and container communications
Configure the K8's API server Securely
maintaining small container images

How to manage a kubernetes cluster: It provides a command-line tool called kubectl to manage your cluster 

we can use kubectl to send instructions ro cluster control plane or use the API  server to get inforamtion about all kubernetes objects 

Syntax for treminal window: kubectl [command] [TYPE] [NAME] [flags]
 The arguments for the syantax to run Kubectl
"command":  refers to the operation that needs to be performed
"TYPE": Refers to the resource type which is case insensitive
"NAME": refers to the resource name whcih is case sensitive. if the name is omitted details for the all resources will be displayed
"Flags": refers to the optional flags Eg: we can use SR server flag to specify the address  and port of the k8's API server

To initialize the cluster run the following command in the master node: "sudo kubeadm init"