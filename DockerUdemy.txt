Docker Intro:
There are three major things in Docker which made this tool a whole evolution in software development.

The Docker Image, Docker Registry and Docker Container.

These three components runs together like bread and butter which made the possibility of Build, Ship and Run anywhere.

The Docker image or the Container image:
(Universal app packaging)
There were many package managers that were brrn used in software packaging but some them were limited to the capabilities.
It resulted that a software packaging manager which has a feature of Cross OS, Cross Platform and application language agnostic,
means it will run everywhere, and it runs on anything, as long is running linux or windows.

DockerFile:
A Dockerfile is a set of instructions to make a container image. The Docker image specification was so popular it's now a standard,
Known as OCR image standard.

Every Docker file starts with FROM instruction, it can be start with someone else's image. 
we can also have many images from many FROM's.
Eg: 
FROM pyton           ------>python bins+libs
RUN pip install flask------> adds flask
WORKDIR /app----> meta data change in the filesystem. 
COPY .. ----> this command copies the source code from the host we're building into its own layer stacked on top
              of the others        
CMD python app.py   ------->

The docker conists of layers and with the capital letters called stanza. The docker process it from top down, will create layers.
The layers will be moved around the servers as tarballs and inside those layers are files, directories and file permissions, and 
sometimes metadata.

According to the example , docker finds the python image that includes binaries & Libraries.

This way the an image is build using Docker Build command.

THE DOCKER REGISTRY:
It is for application distribution.

Every image has unique SHA hash.

DOCKER CONTAINER RUN:

When we create a dockerfile and build the image we can run a container and this running container is known as a NAMESPACE(This is an Linux Feature in which
it prevents the application running in the container from seeing the rest of the operating system/ for Windows its diffrent Feature.) 

It gets it's own blank file system and it will includes only the files that were in that image being built.

It will gets its own IPAddress,ProcessList and Virtual NIC.

When we do the Docker run command , it the image not present locally, then the Docker downloads the image automatically.

MAJOR LINUX FEARTURES USED BY DOCKER:

NAMESPACES-, CGROUPS, VETH(virtual Ethernet), IPTABLES-routing and control interface, 
UNIONMOUNT- to get the file system with all the layers in image in it, into the container at start to took(it is like isolated file system from rest of the host). 


Why now Docker?

The Isolation Problem:

Docker gives isolation like VM's, It allows to maintain multiple versions of same application in the same host.

The Problem of environments:

Once the Vm became more handy it increased the need no of environments to test in operations, production and user device. 

Every part in this for particular environment whcih needs the different configurations 

The container image itself became a abstract or a contract between where its run and whats running inside it.

The Problem of Speed,

Taking about Business, the docker helps to execute the ideas in then SDL and express the ideas in effective way to customer.

Develop, Build, test and Deploy faster,

Container exists  because: better isolation in single os, reduced evironment variences, increasing the speed of change(SDL and Business)


Containers:

'Docker Version'---> returns the versions of client and server docker engine in their services.

it validates the connection between them.


'Docker info'---> it returns the configuration setup of the engine. Eg: No of containers , images, running, stopped


Images vs Containers:

An Image is the binaries and libraries and source code that all make up the application.

A container is a running instance of that image.
 
We can have many numbers of conatainers using same image.

Eg to run nginx container: Docker container run --publish 80:80 nginx

Step1: Downloaded the latest image 'nginx' from the docker hub.

step2: started a new conatiner from that image.

Step3: opened port 80 on the host IP.

Step4: Routes the traffic to the container IP, port80.

'Docker Container ls'---> list the running containers.
'Docker Container run' ---> starts a new container from an image.

Eg to run nginx container DETACH mode: Docker container run --publish 80:80 --detach nginx

tells the docker to run it in background.

'Docker container stop' <container ID>---> stops the containers process but does not remove it.

'Docker Container ls -a'--->List all the containers running and stopped .

Run Vs Start

'Docker conatiner run' always starts a *new* container.

'Docker container start' used to start an existing stopped one.

By Default, the names that are given by docker are a random ones(if not specified) that are genearted automatically from
an open source list of adjectives and surnames of notable hackers and scientists.
 
Eg to run nginx container in DETACH mode with specific name: Docker container run --publish 80:80 --detach --name webhost nginx
 To run a MYSQL we can use docker run -d -p 3306:3306 --name DB -e MYSQL_RANDOM_ROOT_PASSWORD=YES mysql
'Docker conatiner logs' <ID> or <name>
to list the logs of containers.

what happens when 'Docker Container run'

It looks for the image that was specified at the end of the command {eg: niginx}, looks for the image 
locally (it looks it in the image cache)if not available it will pull it from the registry.

It downloades the latest version if the version is not specified.

then it will creats a new container based on the image, it actually going to start a new layers of changes, right on top of where the image left off,

then it will customise the networking, it gives a specific Ip address thats inside a docker virtual network 

then it will open up the port that specified, if the ports are didn't mentioned using --publish then it not open any ports. Eg: 80:80 
opens the ports 80 on host & forwards to port 80 in container

then the container will start the CMD specified in the docker file.


What a container do:(to check the behavior)

'Docker container top'---> returns the list running process in specific container.

'ps aux'--> it returns the process running in linux machine.

'Docker container inspect'--> shows the metadata about the container (startup config, volumes, networking, etc)

'docker container stats'---> it shows the performance data of all containers.

Getting inside a container:

'Docker container run -it'

where, -t for allocating a pseudo -tty(simulates a real terminal, like what SSH does)
       -i interactive (keep the session open to receive terminal input)

to run a container in interactive mode:
'Docker run -it --name proxy nginx bash'

here the 'bash' is specified at the end is nothing but we are passing command or arguments telling what the container to do when it starts up.

bash shell--> if run with -it, it will give you a terminal inside the running container.
 it takes to the inside of the container as root user

Also we can run a container in interactive mode while we giving run command in it and we can do adminstrative work inside the container and 
download packages and update within the container but the resources will be mininmal. for example we can run in interactive mode on a 
ubuntu container by pulling the image and then we can install curl in it and it will work like local machine.

once we stop the conatiner it will not shows in LS and if we want start that container in interactive mode we need to use'docker container start -ai <name con>'

if we want to see the shell inside a running container that actually running something like MYSQL or NGINX we can use 'docker container exec -it <name of con>'
[Run addtional process in running container ], this helps to run additional process in an running conatainer without interrupting the primary process

Eg: while troubleshooting

Using ALPINE:

Alpine is a small security focused distribution of linux.

docker container run -it alpine bash---> it will return bash isn't in side the container 

so,this proves that we can run only things in the container that already exists in image 

In alpine the bash does not exists, instead it has sh(shell)

  
Docker Networks: 

Review: "-p " which exposes the port on your machine.

Docker Conatiner Defaults:

When a container is getting started, in background it is getting connected to a particular Docker Network(Private Virtual network).

By Default, it connects to "Bridge Network".

Then each one of those networks that are connected to actually route through a NAT firewall on Host IP , whcich actually the docker

Daemon Configuring the host Ip address on its default  interface so that your container can get out the internet or to the rest if the
network and then get back

All the containers on a virtual network can talk to each other without -p
 
Eg: if a application had an SQL server and PHP Apache container , those  two containers should be on the same network and 
they are able to talk to each other without actually opening their ports to the rest of the physical network.

And if the an another application unrelated, that was using Mongo and node.js, we could create network for that so that they could
talk with each other without using the "-p" to expose them to the network. but they couldn't actually talk to the other network
where you might have an unrelated app running.

it is like "Batteries included but removable:)"
 
Defaults work in most of the cases but easy to swap out parts to customise it.

The -p/ --publish is publishing the ports is always in HOST:CONTAINER format.

"Docker container port <container name/ID>" ----> shows the ports forwarding traffic in that container from the 
host to the container itself.

The container will not use the ip address of the host itself, by default we can use inspect to figure out, using "format" option

"--format"----> command option for formatting the output of commands using "GO templates"

to know the Ip address of the container "docker container inspect --format '{{.NetworkSettings.IPAdress}}'<container name>"(if it is windows 
we need to use double qoutes rather than single quotes in --format)

this is actual node of that json output that will get..

The output (ipaddress) will not be same as host IP.

In docker we cant make a container to listen on more than one port at the host level, only one can listen.

Containers can talk to each other inside there own virtual network unless they are connected in the same VN.

If they the containers are connected to different VNs and wants to talk to each other then they need to go via their
listening IP all they out the physical network again coming into the firewall then to ethernet interface then to the 
another VN.

Docker Networks: CLI Management of Virtual networks:

"Docker Network ls"--> Shows the list of networks (spawns a new virtual network for the container that we want to connect.)

"docker network inspect"---> to inspect a network

"docker network create --driver" ----> Create command that has an optional driver that we can specify for using built-in
and third party drivers to create a new virtual network.

"Docker network connect"----> Attach a network to container.(Dyanmically creates a NIC in a container on an existing VN)

"Docker network disconnect" ----> for detach a network from container, so that a new NIC is created on a virtual network for that
container.(Dyanmically removes a NIC in a container on an specific VN)

"Bridge" network in  docker which is a default network that bridges through NAT firewall to the phyical network that the host 
is connected to....
    "--network bridge"---> Default docker VN, which is NATed behind host IP.

"Host" network is a special network that skips the virtual networking of docker and attaches the container directly to the 
host interface.
    "--network host"---> it gains performance by skipping virtual networks but sacrifies security of container model.
it has its own pros and cons be it actually prevents the security boundaries of the containerization from protecting 
the interface of that container.

Also it can increase the perfromance of high throughput networking.

"NONE" Network of having an inteface on the computer thats not attached to anything, 

    "--network none" removes eth0 and only leaves with localhost interface in container.


A network driver created a virtual network locally with its own subnet somewhere around 172.17 and above...

   
Points of Docker Networking: Default Security

Create the apps so the FE/BE sit on same docker network.
Their Inter-communication never leaves host.
All externally exposed ports closed by default.
We must manually expose via -p, which is better default security.

Docker Networks: DNS

There is one cruial thing to all these containers and virtual networks and them taking to each other, and thats naming.
because in the world of containers constantly launching and disappering & moving & expanding & shrinking and of these 
microservices that we're seeing cropup everywhere, is that we no longer rely in IP address as the way to talk (Forget Ips)
from one thing to other. Because we can't assume from minute that IP addresses are may even going to be same, the container
might go away ot it might fail and then docker brings it up somewhere else. 

It just too dyanamic and too complicated to deal with that.

but there is solution for this and that is DNS naming.

Docker DNS: Docker daemon has a built -in DNS server that container use by default,
 The container names are equivalent of a host name for containers taking to each other.

When we are creating a new network not using default ones to run the conatiners it actually gets a new feature,
which is automatic DNS resolution for all the containers on that VN from all the other containers on that virtual
network using their container names.

DNS Default names 

Docker defaults the hostname to the container's name but we can also set aliases.

when we create two containers in same VN that can able to communicate with each other with their names,

this solves the huge problem when we are spinning up containers because we can't predict how long they last and
where they might be a minute from now,

In a production design we got a cluster of Docker swarm servers.

It may not change very much on the local machine, but if we stop 3 or 4 containers, and then start the same container, and if start them in a different order,
they may not have the same IP address.

But their host names, or their container names, will always be the same.

The Default bridge network has one disadvantage  that it does not have the DNS server built into it by default.
instead we need to use --link to connect mannualy in that default bridge network,

but if we create a new network for the apps it makes easier and we don't want to link procedure.

Docker CLI assignment

docker conatiner run --rm -it centos:7 bash

yum update curl

curl --version 


Docker DNS Round Robin assingment

docker network create Dude

docker run -d --net dude --net-alias search elasticsearch:2

docker container run --rm -net Dude alpine nslookup search

docker container run --rm  --net Dude centos curl -s search:9200



Container Images

What is an Image(And what isn't)

Its an application binaries and dependencies for the app and the metadata on how to run it.
 
Official Definition: An image is an ordered collection of root filesystem changes and the corresponding execution parameters for
use within a container runtime.

Inside an image there is no complete OS, there is no kernel, no kernel modules(e.g drivers)

It contains just the binaries that the application needs because the host provides the kernel

The Image is very small, it can be a single file(Small as one file (the app binary) like a golang static binary)

for instance, using Go, one of Go's features is that it can build a static binary and have a single file as your application

or we can have a very big image thats actually using some distribution like Ubuntu with its own package manager built in, and where we installed
Apache, and PHP, and the source code and all the all the added modules we need. It can be in GB's


Docker hub registry images:

It is good to use official Images.

the official images are maintained and documented correctly and the versions also will be updated regularly.

The Versions are the common features of official repositories.

When we start to talk about image it not named they are tagged.

A version of an image can actullay have more than one tag.

The image ID is based upon the cryptograpic SHA of each image in Docker Hub


Images and Their Layers:

Images are designed using the union file system concept of making layers about the changes.

"Docker image history <image>"

shows the layers if changes made in image.

When we run the command it actually shows some list, which is not as list of things that have actually happened in the container
because this is about an image.

It is a history of image layers.

Every image starts from the very beginning with a blank layer known as scratch.

Then every set of change that happens after that on the file system, in the image, is another layer.

it might have one layer, it might have dozens of layers and some layers maybe no change in terms of file size.

and we can also notice that the image can have a change that was simply a metadata change about which command we are actually going
to run.

When we are creating a new image it will starts with one layer...

Every layer in an image gets its own unique SHA ID that helps the system to identify, if that layer is the same or another layer

if we are create a two image with same base immage then changes made in those images will be maintained on the top of the same base layer.

so this eliminates the creation of additional layers for the existing base image an using cache.

This is the fundamental concept of the cache which saves a bunch of time and space.

docker knows how to match the difference between the dockerhub registry and the local cache.

As we make changes in the image it creates more layers.

one of the benifits of the syste, that we are never storing the same image data more than once on our file system.

It also reduces the necessity of dowloading and uploading the samw layers already we have on the other side.

for creating custom images and if we are creating two websites A & Website B using same base image then we will ended up with two 
different images

How the image run as container:

Suppose if we have an image and we decide to run a container off of it, then docker does it by creating a new Read/Write layer for that
container on top of the base image.

when we perusing the file system and these things, all the container and images look like regular file system, but underneath
the storage driver that used by the docker it is actually layering all the changes on top of each other.

so if we ran two containers using same base image in terms of the file space , there would only be the differencing between whats
happened on that live container running and whats is happening in the base image, which is read only.

Bottom line is if we are running containers and changing files that were coming through the image and if we change a file that was in the running
container.This is known as Copy on Write 

COW--> Copy On Write:

What it does is if we make change in the file sytem in the image it will take that file out of the image and copy it into the differencing of the
image and store copy of the file in that container layer.


Image Inspect:

Docker image inspect <image ID>--> returns JSON metadata about the image.

It actually returns the metadata of the images and the default configurations.

it has the details& Configurations of image which can run in default mode.


Image tagging and pushing to Docker Hub:

"Dokcer image tag" -----> assign one or more tags to image

Images don't technically have name.

The repository is made up of user name or the organization name slash repo

Official Repositories : they live at the "roof namespace" of the registry, so they don't need to account name front of repo name.
 
The tag is for pointing to specific image commit and it could be anything, into that Repo.

suppose if an image is pulled from registry which has two or more tags an in the host we have pulled,
one of the tagged image then if we try to pull another image which has another tag with same repo then 
docker knew that the based in the image ID , the image is already in the cache, if we pulled that tag we can see
the two same images with different tags but with same image ID so the pull is not stored twice in the cache again.

So these tags are the labels that point to an actual image ID and we can have many of them all point to the same one.

we can also retag existing image, the command as below

"Dokcer image tag<image that we are going to give new tag>/<the actual new tag that we want to give to it>" if tag is not provided,
it will point the latest version of software.

"Latest"---> it's just the default tag, but image owners should assign it to the newest stable version.

"Docker image push"----> uploads changed layers to a image registry.

"Docker login <server>"----> defaults to logging in HUB, but it can overrided by adding server URL.

how docker does this? ,when ever we try to login in hub, well actually it will have local authentication key in config.json 
file that will allow the local docker CLI to access docker hub.

"Docker logout"-----> always logout from shared machines or servers when done, to protect the account.


Building Images: The dockerfile basics

Docker is not like shell script or batch file,

FROM command: It is normally a minimal distibution reason for using this to save time, because these minimal,
distributions are much smaller than CD's we would actually use Vm from them.

Package manager: PMs like apt and yum are one of the reasons to build containers

FROM Debian, Ubuntu, Fedora or CentOS

ENV--> Environment Variabels: which are actually important in containers because they are the main way we set keys and values
for container building and for running containers

One Reason they are chosen as preffered way to inject key/value is they work everywhere, on ever OS config.

RUN--->executing shell commands inside the container as it building.

 adding "&&"(double ampherstand) is to chain the shell command to reduce the number layers created while building the image.

EXPOSE ----> by default bo TCP or UDP ports are open inside the container.
it does not expose anything from the container to a virtual network unless we list it here.

CMD--->this is the final command that will be running every time when we launch a new container from the image, or 
everytime you restart a stopped container.

Ordering the Dockerfile
when we are creating a dockerfile we need to keep the most commands that changes less operations and at the bottom we can run operations that do more.

Extending official images:


 








 





